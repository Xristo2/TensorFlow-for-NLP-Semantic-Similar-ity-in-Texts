{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Semantic_Similarity_in_Texts.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl9zNz0-tSNj"
      },
      "source": [
        "# WELCOME TO **\"TensorFlow for Neural Language Processing\" Series**  üòÅ \n",
        "\n",
        "TensorFlow makes it easy for beginners and experts to create machine learning models for desktop, mobile, web, and cloud. TensorFlow provides a collection of workflows to develop and train models using Python, JavaScript, or Swift, and to easily deploy in the cloud, on-prem, in the browser, or on-device no matter what language you use.\n",
        "\n",
        "We will see how we can gain insights into text data and hands-on on how to use those insights to train NLP models and perform some human mimicking tasks. Let‚Äôs dive in and look at some of the basics of NLP.\n",
        "<br/> <br/>\n",
        "**In this series of 4 project courses, you will learn practically how to build Natural Language Processing algorithms and learn how to create amazing models and build, train, and test Neural Networks in NLP with Tensorflow!** üòé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAeQyfXktSNk"
      },
      "source": [
        "## üëâüèª Course 1: Text Embedding and Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPMwcem6tSNm"
      },
      "source": [
        "## üëâüèª Course 2: Semantic Similarity in Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpBdKtwhtSNp"
      },
      "source": [
        "## üëâüèª Course 3: Sentiment Analysis in Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMls0fPjtSNq"
      },
      "source": [
        "## üëâüèª Course 4: Text Generation with RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L_H5CPgtSNr"
      },
      "source": [
        "print (\"Let's start with Course 1: Word and Text Embeddings\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# WELCOME to this guided project \"Semantic Similarity in Texts\" on Coursera Labs! üòÅ \n",
        "#### This project course is part of \"Tensorflow for Natural Language Processing\" Series of project courses on Coursera.<br/><br/>\n",
        "\n",
        "In this project, we will start coding, and we will go through 5 tasks:<br/> <br/>\n",
        "üëâüèª **Task 1**: Introduction and Overview of the Project. <br/><br/>\n",
        "üëâüèª **Task 2**: Import Libraries and Create Text Representations. <br/><br/>\n",
        "üëâüèª **Task 3**: Create and Visualize Semantic Similarity. <br/><br/>\n",
        "üëâüèª **Task 4**: Download the Data for Semantic Similarity. <br/><br/>\n",
        "üëâüèª **Task 5**: Evaluate the Semantic Textual Similarity. <br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAVQGidpL8v5"
      },
      "source": [
        "## üëâüèª Task 1: Introduction and Overview of the Project\n",
        "This project illustrates how to access the Universal Sentence Encoder and use it for sentence similarity and sentence classification tasks. üåå\n",
        "<br/>\n",
        "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. üî§<br/>\n",
        "The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data. <br/>\n",
        "\n",
        "At the end of this project, you will try out an amazing Bonus Exercise! ü§©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Grt8_antSN1"
      },
      "source": [
        "## üëâüèª Task 2: Import Libraries and Create Text Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "**Import Libraries**\n",
        "\n",
        "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs.üî§"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:07:50.889235Z",
          "iopub.status.busy": "2020-10-02T12:07:50.888615Z",
          "iopub.status.idle": "2020-10-02T12:07:52.286986Z",
          "shell.execute_reply": "2020-10-02T12:07:52.286286Z"
        },
        "id": "lVjNK8shFKOC"
      },
      "source": [
        "%%capture\n",
        "!pip3 install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2020-10-02T12:07:52.293393Z",
          "iopub.status.busy": "2020-10-02T12:07:52.292681Z",
          "iopub.status.idle": "2020-10-02T12:08:19.650605Z",
          "shell.execute_reply": "2020-10-02T12:08:19.651101Z"
        },
        "id": "zwty8Z6mAkdV"
      },
      "source": [
        "#@title Load the Universal Sentence Encoder's TF Hub module\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:08:19.662676Z",
          "iopub.status.busy": "2020-10-02T12:08:19.661172Z",
          "iopub.status.idle": "2020-10-02T12:08:20.163470Z",
          "shell.execute_reply": "2020-10-02T12:08:20.162800Z"
        },
        "id": "Q8F4LNGFqOiq"
      },
      "source": [
        "#@title Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "message_embeddings = embed (messages) ### YOUR CODE HERE\n",
        "\n",
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "  print(\"Message: {}\".format(messages[i]))\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "  message_embedding_snippet = \", \".join(\n",
        "      (str(x) for x in message_embedding[:3]))\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnvjATdy64eR"
      },
      "source": [
        "## üëâüèª Task 3: Create and Visualize Semantic Similarity\n",
        "\n",
        "The embeddings produced by the Universal Sentence Encoder are approximately normalized.<br/>\n",
        "The semantic similarity of two sentences can be trivially computed as the inner product of the encodings. üë®üèΩ‚Äçü§ù‚Äçüë®üèΩ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqzB4OaRtSON"
      },
      "source": [
        "### Create the Semantic Textual Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:08:20.169442Z",
          "iopub.status.busy": "2020-10-02T12:08:20.168770Z",
          "iopub.status.idle": "2020-10-02T12:08:20.170626Z",
          "shell.execute_reply": "2020-10-02T12:08:20.171046Z"
        },
        "id": "h1FFCTKm7ba4"
      },
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels= labels, ### YOUR CODE HERE\n",
        "      yticklabels= labels, ### YOUR CODE HERE\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339tuJ5Pwqqv"
      },
      "source": [
        "### Visualize the Similarity \n",
        "Here we show the similarity in a heat map. üó∫ <br/>\n",
        "The final graph is a 9x9 matrix where each entry `[i, j]` is colored based on the inner product of the encodings for sentence `i` and `j`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:08:20.176100Z",
          "iopub.status.busy": "2020-10-02T12:08:20.175413Z",
          "iopub.status.idle": "2020-10-02T12:08:20.555087Z",
          "shell.execute_reply": "2020-10-02T12:08:20.555584Z"
        },
        "id": "cPMCaxrZwp7t"
      },
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "run_and_plot(messages) ### YOUR CODE HERE \n",
        "               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjdeCqPJeg-"
      },
      "source": [
        "## üëâüèª Task 4: Download the Data for Semantic Similarity\n",
        "\n",
        "The **STS Benchmark** provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements.‚öñ <br/>\n",
        "The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. ‚úåüèª<br/>\n",
        "Pearson correlation is then used to evaluate the quality of the machine similarity scores against human judgements. ‚öñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5nuBbI1iFQR"
      },
      "source": [
        "### ‚≠êDownload data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:08:20.562754Z",
          "iopub.status.busy": "2020-10-02T12:08:20.562020Z",
          "iopub.status.idle": "2020-10-02T12:08:21.719131Z",
          "shell.execute_reply": "2020-10-02T12:08:21.719618Z"
        },
        "id": "VOs8ZfOnJeBF"
      },
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "import csv\n",
        "\n",
        "sts_dataset = tf.keras.utils.get_file(\n",
        "    fname=\"Stsbenchmark.tar.gz\",\n",
        "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "    extract=True)\n",
        "sts_dev = pandas.read_table(\n",
        "    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "sts_test = pandas.read_table(\n",
        "    os.path.join(\n",
        "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "# cleanup some NaN values in sts_dev\n",
        "sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OKy8WhnKRe_"
      },
      "source": [
        "## üëâüèª Task 5: Evaluate the Semantic Textual Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsQNyQTKtSOj"
      },
      "source": [
        "‚≠ê Lets now evaluate the semantic textual embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:08:21.729782Z",
          "iopub.status.busy": "2020-10-02T12:08:21.728844Z",
          "iopub.status.idle": "2020-10-02T12:08:22.230128Z",
          "shell.execute_reply": "2020-10-02T12:08:22.230553Z"
        },
        "id": "W-q2r7jyZGb7"
      },
      "source": [
        "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "def run_sts_benchmark(batch):\n",
        "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n",
        "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n",
        "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
        "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "  scores = 1.0 - tf.acos (clip_cosine_similarities) / math.pi ### YOUR CODE HERE\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  return scores\n",
        "\n",
        "dev_scores = sts_data['sim'].tolist()\n",
        "scores = []\n",
        "for batch in np.array_split(sts_data, 10):\n",
        "  scores.extend(run_sts_benchmark(batch))\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNwE9MWwtSOs"
      },
      "source": [
        "## Bonus: Extra Exercise!\n",
        "_Refresh Your Memory..._ üòã"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi-UEHpbtSOu"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-82c8FtSOz"
      },
      "source": [
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "- keras.layers.SimpleRNN, a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n",
        "\n",
        "- keras.layers.GRU, first proposed in Cho et al., 2014.\n",
        "\n",
        "- keras.layers.LSTM, first proposed in Hochreiter & Schmidhuber, 1997.\n",
        "\n",
        "In early 2015, Keras had the first reusable open-source Python implementations of LSTM and GRU.\n",
        "\n",
        "Here is a simple example of a Sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbvS4B1OtSO2"
      },
      "source": [
        "#creating the RNN model\n",
        "\n",
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFM-9tXOtSO8"
      },
      "source": [
        "By default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is (batch_size, units) where units corresponds to the units argument passed to the layer's constructor.\n",
        "\n",
        "A RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences=True. The shape of this output is (batch_size, timesteps, units)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8zB38TtSO-"
      },
      "source": [
        "# Outputs and states in RNN\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp1nqivetSPH"
      },
      "source": [
        "In addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\n",
        "\n",
        "To configure a RNN layer to return its internal state, set the return_state parameter to True when creating the layer. Note that LSTM has 2 state tensors, but GRU only has one.\n",
        "\n",
        "To configure the initial state of the layer, just call the layer with additional keyword argument initial_state. Note that the shape of the state needs to match the unit size of the layer, like in the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGGPl6rtSPJ"
      },
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return states in addition to output\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the 2 states to a new LSTM layer, as initial state\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP7ih1IxtSPP"
      },
      "source": [
        "Which of the following is true? ü§î <br/>\n",
        "\n",
        "i) On average, neural networks have higher computational rates than conventional computers.\n",
        "ii) Neural networks learn by example.\n",
        "iii) Neural networks mimic the way the human brain works.\n",
        "<br/>\n",
        "a) All of the mentioned are true? <br/>\n",
        "b) (ii) and (iii) are true?  <br/>\n",
        "c) (i), (ii) and (iii) are true?  <br/>\n",
        "d) None of the mentioned?  <br/>\n",
        "<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> .<br/> . <br/> ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Bn4IIXtSPQ"
      },
      "source": [
        "a)‚úî <br/>\n",
        "‚ú® Neural networks have higher computational rates than conventional computers because a lot of the operation is done in parallel. That is not the case when the neural network is simulated on a computer. The idea behind neural nets is based on the way the human brain works. Neural nets cannot be programmed, they can only learn by examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wr4hRFztSPR"
      },
      "source": [
        "# CONGRATULATIONS! ü§©"
      ]
    }
  ]
}